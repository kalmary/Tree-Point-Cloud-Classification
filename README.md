![BANNER](https://github.com/kalmary/RandLANet_Segmentation/blob/readme-preparation/img/Banner.png)

# Table of contents
1. [Overview](#overview)
2. [Instalation](#installation)
3. [Folder Structure](#fstructure)
4. [Usage](#usage)
5. [Main piplines after training](#piplines)
6. [Citation](#citation)

--- 
# 1. Overview <a name="overview"></a>

**RandLANet_Segmentation** is a set of tools for point cloud semantic segmentation using the RandLANet architecture. RandLANet is a deep learning model designed to process large-scale point clouds. Its architecture, which utilizes a random sampling strateg. The repository includes tools for defining the model, training it, and performing segmentation on new files. Key Features:
- Data preprocessing: cut, decimate and distribite data for training model,
- Model definition: necessary code to define and build the RandLANet model architecture, allows for scalability and adjustment for hardware-specific needs,
- Training & Evaluation: Tools for training the model on custom datasets and evaluating its performance,
- Inference & Segmentation: Utility to perform semantic segmentation on new point cloud files using pre-trained models/ segmentation on preloaded arrays.



Our model is based on:
- https://github.com/QingyongHu/RandLA-Net
- https://github.com/aRI0U/RandLA-Net-pytorch


![IMG](https://github.com/kalmary/RandLANet_Segmentation/blob/readme-preparation/img/RandLANet_scheme.png)


Key modifications we added:
- like memory efficient, gpu - based knn search,
- model configurability from .json file,
- better decoder upsampling.


---

# 1. Instalation: <a name="installation"></a>

```bash

# Clone the repository to your local machine:

git clone https://github.com/kalmary/RandLANet_Segmentation.git

cd RandLANet_Segmentation

# Create and activate a Virtual Environment:
python -m venv .venv
source .venv/bin/activate

# Install all requirements, without pytorch and cuda
pip install requirements.txt

# Tested on this, but should work with any other version
pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128

# update git submodules
git submodule update --init --recursive
```

---

# 2. Folder structure: <a name="fstructure"></a>

```
.
├── data_processing
│   └── downsample_LAZ.py        #Program for preprocessing point cloud files. Creates hdf5 files for training, validation and testing
├── final_files                  #Folder prepared for trained models and their config files
├── main_logs                    
├── main.py                      
├── model_pipeline
│   ├── TrainSegmAutomated.py    #Main Python Program for training 
│   ├── EvalSegm_RandLANet.py    #Main Python Program for evaluating outputs
│   ├── RandLANet_CB.py          #RandLANet model, configurable from dictionary
│   ├── model_configs            #Folder containing all architecture config files
│   ├── training_configs         #Folder containign all parameters config files
│   ├── training_results         #Folder containing all files generated by training and evalutaion files
└── utils
    ├── nn_utils                 #nn_utils (external link)     
    └── pcd_manipulation.py      #Point clound manipulations

```

# 3. Usage <a name="usage"></a>
## 3.1 Preprocessing

Before training a model, data preprocessing must me done. To do so run:
```bash
python src/data_processing/downsample_LAZ.py --source_path path/to/raw/data --decimated_path path/to/decimated/pcds --converted_path path/to/final/processed/files
```
Paths used when processing:
- source_path: directory with raw (.LAZ by default) point clouds,
- decimated_path: serves as a checkpoint. Files are cut, decimated and saved as .npy files,
- converted_path: final directory with processed files, distributed into training/ validation/ testing datasets, chunked into .h5 files,

For more guidance/ guidance when running code, run it with ``--help`` flag.

## 3.2 Training

```bash
cd src/model_pipeline
python TrainSegmAutomated.py --help
```

Output of --help


```bash
Script for training the model based on predefined range of scenarios

options:
  -h, --help            show this help message and exit
  --model_name MODEL_NAME
                        Base of the model's name.
                        When iterating, name also gets an ID. 
                        If not given, defaults to: ResNet_0.
  --device {cpu,cuda,gpu}
                        Device for tensor based computation.
                        Pick 'cpu' or 'cuda'/ 'gpu'.
                        Device for tensor based computation.
  --mode {0,1,2,3,4}    
                        Pick:
                        0: test
                        1: single training
                        2: multiple trainings, grid_based
                        3: multiple trainings, with optuna
                        4: only check models
```

Training mode:

* **Test** - This mode uses a dummy model to check if the overall system and configuration are working correctly.
* **Single training** - Performs a single training run using a specified model and defined configuration settings.
* **Multiple training grid_based** - Executes multiple training runs for hyperparameter tuning by systematically testing all combinations defined in a grid search configuration.
* **Multiple training optuna** - Performs advanced hyperparameter optimization using Optuna. This mode can efficiently find a better optimal solution, often taking more time than a simple grid search. You can also utilize the Optuna Dashboard for visualization and monitoring.
* **Only check models** - This mode check if model compiles.

---

For evaluating:


```bash
cd src/model_pipeline
python EvalSegm_RandLANet.py --help
```
Output of --help

```bash

```


---

# 4. Main piplines after training <a name="piplines"></a>


| Pipline for processing point clouds in arrays | Pipline for processing laz files |
| :---: | :---: |
| Using main.py.... | Opis 2 |
---
# 6. Citation <a name="citation"></a>












