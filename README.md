![BANNER](https://github.com/kalmary/RandLANet_Segmentation/blob/readme-preparation/img/Banner.png)

# Table of contents
1. [Overview](#overview)
2. [Installation](#installation)
3. [Repository Structure](#fstructure)
4. [Usage](#usage)
5. [Main piplines after training](#piplines)
6. [Citation](#citation)

--- 
# 1. Overview <a name="overview"></a>

# 1. Overview <a name="overview"></a>

**RandLANet_Segmentation** is a set of tools for point cloud semantic segmentation using the RandLANet architecture. RandLANet is a deep learning model designed to process large-scale point clouds. Its architecture, which utilizes a random sampling strateg. The repository includes tools for defining the model, training it, and performing segmentation on new files. Key Features:
- Data preprocessing: cut, decimate and distribite data for training model,
- Model definition: necessary code to define and build the RandLANet model architecture, allows for scalability and adjustment for hardware-specific needs,
- Training & Evaluation: Tools for training the model on custom datasets and evaluating its performance,
- Inference & Segmentation: Utility to perform semantic segmentation on new point cloud files using pre-trained models/ segmentation on preloaded arrays.



Our model is based on:
- https://github.com/QingyongHu/RandLA-Net
- https://github.com/aRI0U/RandLA-Net-pytorch


![IMG](https://github.com/kalmary/RandLANet_Segmentation/blob/readme-preparation/img/RandLANet_scheme.png)


Key modifications we added:
- like memory efficient, gpu - based knn search,
- model configurability from .json file,
- better decoder upsampling.

---
# 2. Repository structure: <a name="fstructure"></a>

```
.
├── data_processing
│   └── downsample_LAZ.py        #Program for preprocessing point cloud files. Creates hdf5 files for training, validation and testing
├── final_files                  #Folder prepared for trained models and their config files
├── main_logs                    
├── main.py                      
├── model_pipeline
│   ├── TrainSegmAutomated.py    #Main Python Program for training 
│   ├── EvalSegm_RandLANet.py    #Main Python Program for evaluating outputs
│   ├── RandLANet_CB.py          #RandLANet model, configurable from dictionary
│   ├── model_configs            #Folder containing all architecture config files
│   ├── training_configs         #Folder containign all parameters config files
│   ├── training_results         #Folder containing all files generated by training and evalutaion files
└── utils
    ├── nn_utils                 #nn_utils (external link)     
    └── pcd_manipulation.py      #Point clound manipulations

```
---

# 3. Instalation: <a name="installation"></a>

Clone the repository to your local machine:
```bash

git clone https://github.com/kalmary/RandLANet_Segmentation.git

cd RandLANet_Segmentation
```

Create and activate a Virtual Environment and install requirements:
```bash
python -m venv .venv
source .venv/bin/activate

# Install all requirements, without pytorch and cuda
pip install requirements.txt

# Tested on this, but should work with any other version
pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128

# update git submodules
git submodule update --init --recursive
```

---

# 4. Usage <a name="usage"></a>
## 4.1 Preprocessing

Before training a model, data preprocessing must be done. To do so run:
```bash
python src/data_processing/downsample_LAZ.py --source_path path/to/raw/data --decimated_path path/to/decimated/pcds --converted_path path/to/final/processed/files
```
Paths used when processing:
- source_path: directory with raw (.LAZ by default) point clouds,
- decimated_path: serves as a checkpoint. Files are cut, decimated and saved as .npy files,
- converted_path: final directory with processed files, distributed into training/ validation/ testing datasets, chunked into .h5 files,

For more guidance/ guidance when running code, run it with ``--help`` flag.

## 4.2 Training
Examine contents of:
- ``src/model_pipeline/model_configs`` - .json files with model architectures,
- ``src/model_pipeline/training_configs`` - .json files with training configs.
Pay attention to above files and adjust them to ensure the fit with your available resources.
  
Files with `_single` suffix are meant for single training without any optimizations. Theese are a good choice if fast/ based on known optimal parameters training is required.
For most optimal results we recommend using options based on multidimensional space of hyperparameters:
- grid based - only with optimization case is really simple/ configs combination number is low,
- Optuna library based - for high dimensional and complex cases. If you have special needs in terms of time computation,
you can change ``n_trials`` in ``main`` function of ``TrainSegmAutomated.py``:
```python
optuna_based_training(exp_config=exp_configs,
                      model_name=model_name,
                      n_trials=80)

```
``n_trials`` is what we found to be giving good, repeatable results, but lower numbers where also acceptable.

To start training run:
```bash
cd src/model_pipeline
python src/model_pipeline/TrainSegmAutomated.py --model_name MODEL_NAME --device cuda --mode 3
```
Available flags:
- ``model_name`` - name of your model. Results are stored in ``src/model_pipeline/training_results/MODEL_NAME``,
- ``device`` - we recommend training on strong GPUs (tested on RTX 5090), but any CUDA enabled one with at least 16 GB of VRAM (smaller models), will work fine,
- ``mode`` - you can choose following:
    - `0` - testing mode: ,
    - `1` ,
    - `2` ,
    - `3` ,
    - `4` - check models - run this to get a rough idea of model resource demands.

Output of --help


```bash
Script for training the model based on predefined range of scenarios

options:
  -h, --help            show this help message and exit
  --model_name MODEL_NAME
                        Base of the model's name.
                        When iterating, name also gets an ID. 
                        If not given, defaults to: ResNet_0.
  --device {cpu,cuda,gpu}
                        Device for tensor based computation.
                        Pick 'cpu' or 'cuda'/ 'gpu'.
                        Device for tensor based computation.
  --mode {0,1,2,3,4}    
                        Pick:
                        0: test
                        1: single training
                        2: multiple trainings, grid_based
                        3: multiple trainings, with optuna
                        4: only check models
```

Training mode:

* **Test** - This mode uses a dummy model to check if the overall system and configuration are working correctly.
* **Single training** - Performs a single training run using a specified model and defined configuration settings.
* **Multiple training grid_based** - Executes multiple training runs for hyperparameter tuning by systematically testing all combinations defined in a grid search configuration.
* **Multiple training optuna** - Performs advanced hyperparameter optimization using Optuna. This mode can efficiently find a better optimal solution, often taking more time than a simple grid search. You can also utilize the Optuna Dashboard for visualization and monitoring.
* **Only check models** - This mode check if model compiles.

---

For evaluating:


```bash
cd src/model_pipeline
python EvalSegm_RandLANet.py --help
```
Output of --help

```bash

```


---

# 4. Main piplines after training <a name="piplines"></a>


| Pipline for processing point clouds in arrays | Pipline for processing laz files |
| :---: | :---: |
| Using main.py.... | Opis 2 |
---
# 6. Citation <a name="citation"></a>













